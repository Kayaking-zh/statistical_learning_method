{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../dataset/train.csv')\n",
    "X, Y = train.drop('label', axis=1).to_numpy(), train['label'].to_numpy()\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.3)\n",
    "test = pd.read_csv('../dataset/test.csv')\n",
    "X_test = test.to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "给定数据集$$T={(x_1,y_1), (x_2,y_2),\\cdots,(x_N,y_N)}$$\n",
    "其中$x_i \\in \\mathbb{R}^n$，$y_i \\in \\{c_1, c_2, \\cdots, c_K\\}$\n",
    "\n",
    "朴素贝叶斯法通过对训练集学习，将实例分到后验概率最大对类中$$y=\\mathop{\\arg\\max_{c_k}}P(y=c_k|X=x)$$\n",
    "根据条件概率公式以及全概率公式可得$$\\mathop{\\arg\\max_{c_k}} P(y=c_k|X=x)=\\mathop{\\arg\\max_{c_k}} \\frac{P(X=x|y=c_k)P(y=c_k)}{\\sum\\limits_{k}P(X=x|Y=c_k)P(Y=c_k)}$$\n",
    "分母对每一类都是相同的，所以可以将分母省略，朴素贝叶斯法通过对条件概率进行条件独立性假设得名，$$P(X=x|Y=c_k)=\\prod_{j=1}^n P(X^{(j)}=x^{(j)}|Y=c_k)$$\n",
    "在具体实现中，为防止概率溢出，通常将概率进行对数处理，故$$y=\\mathop{\\arg\\max_{c_k}} \\log P(y=c_k) + \\sum\\limits_j \\log P(X^{(j)}=x^{(j)}|Y=c_k)$$\n",
    "\n",
    "使用极大似然估计对先验概率以及条件概率进行学习，本例中采用多项式分布，则$$P(y=c_k)=\\frac{\\sum\\limits_{i=1}^n I(y_i=c_k)}{N}$$\n",
    "在学习条件概率时，为避免因训练集无法完全反应真实的数据分布而出现所要估计的概率为0的情况，通常对概率进行Laplace平滑，设第$j$个特征的取值集合为$\\{a_{j1}, \\cdots, a_{jS_j}\\}$，则$$P(X^{(j)}=a_{jl}|Y=c_k)=\\frac{\\sum\\limits_{i=1}^N I(x_i^{(j)}=a_{jl}, y_i=c_k)}{\\sum\\limits_{i=1}^N I(y_i=c_k)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveBayes:\n",
    "    def __init__(self):\n",
    "        self.model = {}\n",
    "    \n",
    "    def count(self, x):\n",
    "        return {\n",
    "            feature: np.log(len(x[x == feature]) + 1) - np.log(len(x) + 256)\n",
    "            for feature in range(256)\n",
    "        }\n",
    "    \n",
    "    def fit(self, X, Y):\n",
    "        X, Y = np.array(X), np.array(Y)\n",
    "        labels = np.unique(Y)\n",
    "        dim = X.shape[1]\n",
    "        \n",
    "        for label in labels:\n",
    "            self.model[label] = {\n",
    "                'feature_log_prior': [],\n",
    "                'class_log_prior': np.log(len(Y[Y == label])) - np.log(len(Y))\n",
    "            }\n",
    "            \n",
    "            for i in range(dim):\n",
    "                self.model[label]['feature_log_prior'].append(\n",
    "                    self.count(np.array([x[i] for x, y in zip(X, Y) if y == label])))\n",
    "    \n",
    "    def predict_one(self, x):\n",
    "        pred, pred_prob = None, 0\n",
    "        \n",
    "        for label in self.model:\n",
    "            prob = self.model[label]['class_log_prior']\n",
    "            \n",
    "            for i in range(len(x)):\n",
    "                prob += self.model[label]['feature_log_prior'][i][x[i]]\n",
    "                \n",
    "            if (prob > pred_prob) or (pred_prob == 0):\n",
    "                pred, pred_prob = label, prob\n",
    "                \n",
    "        return pred\n",
    "    \n",
    "    def predict(self, X):\n",
    "        res = []\n",
    "        \n",
    "        for x in X:\n",
    "            res.append(self.predict_one(x))\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NaiveBayes()\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8217460317460318"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred = model.predict(X_val)\n",
    "accuracy_score(Y_val, Y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "model = MultinomialNB()\n",
    "model.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8311111111111111"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_pred = model.predict(X_val)\n",
    "accuracy_score(Y_val, Y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
